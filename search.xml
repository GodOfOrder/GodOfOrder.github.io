<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Scrapy+Selenium爬取简书网</title>
      <link href="/2019/02/19/Scrapy-Selenium%E7%88%AC%E5%8F%96%E7%AE%80%E4%B9%A6%E7%BD%91/"/>
      <url>/2019/02/19/Scrapy-Selenium%E7%88%AC%E5%8F%96%E7%AE%80%E4%B9%A6%E7%BD%91/</url>
      
        <content type="html"><![CDATA[<blockquote><p>为什么要选取简书网, 因为在爬取该网站数据的时候我发现有些数据是异步加载的, 而且不仅仅是文章的推送, 在文章内部的阅读量、评论数等信息也不是同步加载的,这就类似于很多网页使用的Ajax技术. 鉴于有比较多的数据为异步加载, 所以这里使用Scrapy+selenium实现爬取Ajax数据.</p></blockquote><h4 id="网页分析"><a href="#网页分析" class="headerlink" title="网页分析"></a>网页分析</h4><p>这里使用Chrome浏览器的<code>Toggle JavaScript</code>扩展程序查看网站是否为异步加载, 首先我们disable JavaScript, 网页只会加载7篇文章:</p><p><img src="/2019/02/19/Scrapy-Selenium爬取简书网/1.png" alt=""></p><p>再次启用, 将网页下拉会额外加载7篇:</p><p><img src="/2019/02/19/Scrapy-Selenium爬取简书网/2.png" alt=""></p><p>这么看简书网首页的文章是通过Ajax技术异步加载的. 将首页一直下拉到底, 会加载出来一个”阅读更多”, 点击后会继续加载7篇文章, 那么首先确定思路: 先分析下拉首页自动加载的文章, 再分析点击”阅读更多”额外加载的文章.</p><h4 id="下拉首页异步加载的Ajax"><a href="#下拉首页异步加载的Ajax" class="headerlink" title="下拉首页异步加载的Ajax"></a>下拉首页异步加载的Ajax</h4><p>先将网页下拉到最底:</p><p><img src="/2019/02/19/Scrapy-Selenium爬取简书网/3.png" alt=""></p><p>发现会多出两条以seen_snote 开头的请求, 这是一个GET请求, url结构很明显, 在首页url的后面添加？ 以及一堆字符串, 这些字符串固定是由seen_snote_ids[] + 一串数字组成, 这些数字结构看起来很眼熟啊, 随意复制第一个到网页Element中查看:</p><p><img src="/2019/02/19/Scrapy-Selenium爬取简书网/4.png" alt=""></p><p>果然就是文章的编号, 而且正好是前面7篇文章的编号, 那么问题就简单了: 获取已加载的文章的编号再组成新的url请求隐藏的文章,首先做一个测试:</p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> requests<span class="token keyword">from</span> urllib<span class="token punctuation">.</span>parse <span class="token keyword">import</span> quote<span class="token keyword">from</span> lxml <span class="token keyword">import</span> etreeurl <span class="token operator">=</span> <span class="token string">"https://www.jianshu.com/"</span>headers <span class="token operator">=</span> <span class="token punctuation">{</span>    <span class="token string">'user-agent'</span><span class="token punctuation">:</span>    <span class="token string">'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36'</span><span class="token punctuation">}</span>meta <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>res <span class="token operator">=</span> requests<span class="token punctuation">.</span>get<span class="token punctuation">(</span>url<span class="token punctuation">,</span> headers<span class="token operator">=</span>headers<span class="token punctuation">)</span>source <span class="token operator">=</span> res<span class="token punctuation">.</span>content<span class="token punctuation">.</span>decode<span class="token punctuation">(</span><span class="token string">'utf-8'</span><span class="token punctuation">)</span>htmlElement <span class="token operator">=</span> etree<span class="token punctuation">.</span>HTML<span class="token punctuation">(</span>source<span class="token punctuation">)</span>note_id <span class="token operator">=</span> htmlElement<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">"//li[contains(@id,'note-')]/@data-note-id"</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#获取首页上已加载出来的文章的id, 爬取前三个做比较</span>titles <span class="token operator">=</span> htmlElement<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">"//a[@class='title']/text()"</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#获取文章标题</span>url <span class="token operator">+=</span> <span class="token string">"?"</span><span class="token keyword">for</span> id<span class="token punctuation">,</span>title <span class="token keyword">in</span> zip<span class="token punctuation">(</span>note_id<span class="token punctuation">,</span>titles<span class="token punctuation">)</span><span class="token punctuation">:</span>    meta<span class="token punctuation">.</span>append<span class="token punctuation">(</span>quote<span class="token punctuation">(</span><span class="token string">'seen_snote_ids[]'</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">'='</span> <span class="token operator">+</span> id<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"id=%s title=%s"</span><span class="token operator">%</span><span class="token punctuation">(</span>id<span class="token punctuation">,</span>title<span class="token punctuation">)</span><span class="token punctuation">)</span>meta<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token string">'page=%s'</span><span class="token operator">%</span>str<span class="token punctuation">(</span>len<span class="token punctuation">(</span>meta<span class="token punctuation">)</span><span class="token operator">//</span><span class="token number">7</span> <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"="</span><span class="token operator">*</span><span class="token number">30</span><span class="token punctuation">)</span>url <span class="token operator">=</span> url <span class="token operator">+</span> <span class="token string">'&amp;'</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>meta<span class="token punctuation">)</span>Ajax_res <span class="token operator">=</span> requests<span class="token punctuation">.</span>get<span class="token punctuation">(</span>url<span class="token punctuation">,</span>headers<span class="token operator">=</span>headers<span class="token punctuation">)</span>Ajax_htmlElement <span class="token operator">=</span> etree<span class="token punctuation">.</span>HTML<span class="token punctuation">(</span>Ajax_res<span class="token punctuation">.</span>content<span class="token punctuation">.</span>decode<span class="token punctuation">(</span><span class="token string">'utf-8'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>note_id_ <span class="token operator">=</span> Ajax_htmlElement<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">"//li[contains(@id,'note-')]/@data-note-id"</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true">#获取额外加载的文章id</span>titles_ <span class="token operator">=</span> Ajax_htmlElement<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">"//a[@class='title']/text()"</span><span class="token punctuation">)</span><span class="token keyword">for</span> id<span class="token punctuation">,</span>title <span class="token keyword">in</span> zip<span class="token punctuation">(</span>note_id_<span class="token punctuation">,</span>titles_<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"id=%s title=%s"</span><span class="token operator">%</span><span class="token punctuation">(</span>id<span class="token punctuation">,</span>title<span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><p>输出的结果如下(截取三篇做测试):</p><blockquote><pre><code>id=39374653 title=黄帝内经条文经常被滥用，看这句就被养生的玩坏id=39625784 title=这些天，我做过的文章兼职id=37606409 title=《沙漠骆驼》抄袭死人作品，要脸吗？==============================id=39374653 title=黄帝内经条文经常被滥用，看这句就被养生的玩坏id=39625784 title=这些天，我做过的文章兼职id=37606409 title=《沙漠骆驼》抄袭死人作品，要脸吗？</code></pre></blockquote><p>哎？怎么获取到的是同样的文章?然后对比了一下url, 也没错呀? 看起来问题并没有这么简单,于是我再次仔细地查看了这个Ajax请求:</p><p><img src="/2019/02/19/Scrapy-Selenium爬取简书网/5.png" alt=""></p><p>发现这里的请求头里面有几个’x’开头的字段, 在多次尝试后发现, 这三个值都是不变的, 第二与第三个参数的值应该就是常量, 第一个参数是一长串的字符, 参考之前我的文章, 我判定获取方式应该有两种:<1> 从简书网服务器上获取 <2> 本地网页中获取.  第一种情况可以排除, 因为没有请求的Response结果是返回这个值的, 那么只有第二种情况, 于是我在网页中检索:</2></1></p><p><img src="/2019/02/19/Scrapy-Selenium爬取简书网/6.png" alt=""></p><p>果然是在本地,保险起见, 我将JavaScript disable , 发现这个值依然存在, 好的,那么修改后的代码如下:</p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> requests<span class="token keyword">from</span> urllib<span class="token punctuation">.</span>parse <span class="token keyword">import</span> quote<span class="token keyword">from</span> lxml <span class="token keyword">import</span> etreeurl <span class="token operator">=</span> <span class="token string">"https://www.jianshu.com/"</span>headers <span class="token operator">=</span> <span class="token punctuation">{</span>    <span class="token string">'user-agent'</span><span class="token punctuation">:</span>    <span class="token string">'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36'</span><span class="token punctuation">,</span><span class="token punctuation">}</span>meta <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>res <span class="token operator">=</span> requests<span class="token punctuation">.</span>get<span class="token punctuation">(</span>url<span class="token punctuation">,</span> headers<span class="token operator">=</span>headers<span class="token punctuation">)</span>source <span class="token operator">=</span> res<span class="token punctuation">.</span>content<span class="token punctuation">.</span>decode<span class="token punctuation">(</span><span class="token string">'utf-8'</span><span class="token punctuation">)</span>htmlElement <span class="token operator">=</span> etree<span class="token punctuation">.</span>HTML<span class="token punctuation">(</span>source<span class="token punctuation">)</span>x_csrf_token_value <span class="token operator">=</span> htmlElement<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">"//meta[@name='csrf-token']/@content"</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>note_id <span class="token operator">=</span> htmlElement<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">"//li[contains(@id,'note-')]/@data-note-id"</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span>  <span class="token comment" spellcheck="true">#获取首页上已加载出来的文章的id</span>titles <span class="token operator">=</span> htmlElement<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">"//a[@class='title']/text()"</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">3</span><span class="token punctuation">]</span>url <span class="token operator">+=</span> <span class="token string">"?"</span><span class="token keyword">for</span> id<span class="token punctuation">,</span>title <span class="token keyword">in</span> zip<span class="token punctuation">(</span>note_id<span class="token punctuation">,</span>titles<span class="token punctuation">)</span><span class="token punctuation">:</span>    meta<span class="token punctuation">.</span>append<span class="token punctuation">(</span>quote<span class="token punctuation">(</span><span class="token string">'seen_snote_ids[]'</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token string">'='</span> <span class="token operator">+</span> id<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"id=%s title=%s"</span><span class="token operator">%</span><span class="token punctuation">(</span>id<span class="token punctuation">,</span>title<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"="</span><span class="token operator">*</span><span class="token number">30</span><span class="token punctuation">)</span>headers<span class="token punctuation">[</span><span class="token string">'x-infinitescroll'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">'true'</span>headers<span class="token punctuation">[</span><span class="token string">'x-requested-with'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">'XMLHttpRequest'</span>headers<span class="token punctuation">[</span><span class="token string">'x-csrf-token'</span><span class="token punctuation">]</span> <span class="token operator">=</span> x_csrf_token_valueurl <span class="token operator">=</span> url <span class="token operator">+</span> <span class="token string">'&amp;'</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>meta<span class="token punctuation">)</span>Ajax_res <span class="token operator">=</span> requests<span class="token punctuation">.</span>get<span class="token punctuation">(</span>url<span class="token punctuation">,</span>headers<span class="token operator">=</span>headers<span class="token punctuation">)</span>Ajax_htmlElement <span class="token operator">=</span> etree<span class="token punctuation">.</span>HTML<span class="token punctuation">(</span>Ajax_res<span class="token punctuation">.</span>content<span class="token punctuation">.</span>decode<span class="token punctuation">(</span><span class="token string">'utf-8'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>note_id_ <span class="token operator">=</span> Ajax_htmlElement<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">"//li[contains(@id,'note-')]/@data-note-id"</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true">#获取额外加载的文章id</span>titles <span class="token operator">=</span> Ajax_htmlElement<span class="token punctuation">.</span>xpath<span class="token punctuation">(</span><span class="token string">"//a[@class='title']/text()"</span><span class="token punctuation">)</span><span class="token keyword">for</span> id<span class="token punctuation">,</span>title <span class="token keyword">in</span> zip<span class="token punctuation">(</span>note_id<span class="token punctuation">,</span>titles<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"id=%s title=%s"</span><span class="token operator">%</span><span class="token punctuation">(</span>id<span class="token punctuation">,</span>title<span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><p>输出结果如下(截取前三篇):</p><blockquote><pre><code>id=37871082 title=值得一试的冬日早餐，比被窝更值得眷恋id=38463114 title=我都可以出几本书了id=40700417 title=过年肉吃腻了就多吃这道菜，清爽可口又营养，一端上桌就抢着吃!==============================id=41206930 title=这年头谁还没演过好人呀？《知否》反派代表团，一人一部好剧推荐id=38218039 title=不主动找你，不是因为我不想你id=37242620 title=有趣的男女不同</code></pre></blockquote><p>嗯, id与标题都完全不同,看来这个问题是解决了. 按照相同的分析方法我们也可以获取通过点击’阅读更多’加载的推荐文章.</p><h4 id="Selenium提取文章内部Ajax信息"><a href="#Selenium提取文章内部Ajax信息" class="headerlink" title="Selenium提取文章内部Ajax信息"></a>Selenium提取文章内部Ajax信息</h4><p>任意点开一篇文章, 首先禁用JavaScript, 然后再启用, 发现有几个位置是异步加载的: 文章的阅读量、评论数、喜欢数、赞赏数、收录专题. 好像有点多, 而且加载的位置各异, 这样如果逐条分析就需要大量的请求, 所以这里就采用Selenium直接抓取网页渲染后的结果. 那么网页的分析就完成了,接下来就是在Scrapy内部分布代码了.</p><h4 id="Scrapy框架Spiders部分"><a href="#Scrapy框架Spiders部分" class="headerlink" title="Scrapy框架Spiders部分"></a>Scrapy框架Spiders部分</h4><p>打开命令行执行命令创建项目:</p><blockquote><p>scrapy startproject jianshu_web</p></blockquote><p>然后进入该项目所在路径创建爬虫:</p><blockquote><p>scrapy genspider js “jianshu.com”</p></blockquote><p>完成后找到项目中生成的js.py文件, 我们的思路是首先要获取首页的前7篇文章并获取其编号, 才能后续爬取异步加载的数据,我们知道在Spiders模块中第一条请求是在内置的<code>start_requests()</code>方法中发送的,所以这里重写该方法并为Request请求添加一个回调函数parse_Ajax(),示例如下:</p><pre class=" language-python"><code class="language-python">start_urls <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'https://www.jianshu.com/'</span><span class="token punctuation">]</span><span class="token keyword">def</span> <span class="token function">start_requests</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">for</span> url <span class="token keyword">in</span> self<span class="token punctuation">.</span>start_urls<span class="token punctuation">:</span>        <span class="token keyword">yield</span> scrapy<span class="token punctuation">.</span>Request<span class="token punctuation">(</span>url<span class="token punctuation">,</span>dont_filter<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>callback<span class="token operator">=</span>self<span class="token punctuation">.</span>parse_Ajax<span class="token punctuation">,</span>                             meta<span class="token operator">=</span><span class="token punctuation">{</span><span class="token string">"Function"</span><span class="token punctuation">:</span><span class="token string">"Ajax"</span><span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#这里解析Ajax网页需要的参数</span><span class="token keyword">def</span> <span class="token function">parse_Ajax</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> response<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true">#...</span></code></pre><p>这里我额外地传递了一个meta,后面有用处. 设置<code>dont_filter=True</code> 是告诉调度器不过滤该请求.</p><p>在parse_Ajax() 方法中截取到文章的id并组成新的url, 再保存异步加载出来的文章url, 通过额外的Request请求爬取文章细节信息.</p><h4 id="构建自己的Selenium下载器中间件"><a href="#构建自己的Selenium下载器中间件" class="headerlink" title="构建自己的Selenium下载器中间件"></a>构建自己的Selenium下载器中间件</h4><p>我们都知道Selenium是让机器来模仿人对浏览器进行操作. 那么对于单篇文章内部细节的爬取, 我们就不需要将Request请求发送到<code>DOWNLOADER</code>模块使其从网页上下载数据了. 也就是说, 我们需要在我们自定义的<code>MiddleWare</code>中截取这个爬取文章内容的Request请求, 再将渲染完成的网页源码包装为<code>Response对象</code>返回给Spiders.</p><p>这里查阅<a href="https://docs.scrapy.org/en/latest/topics/downloader-middleware.html" target="_blank" rel="noopener">Scrapy官方文档</a>是这么说明的:</p><blockquote><p><em>class</em><code>scrapy.downloadermiddlewares.`</code>DownloaderMiddleware`</p><ul><li><p><code>process_request</code>(<em>request</em>, <em>spider</em>)</p><p>This method is called for each request that goes through the download middleware.<a href="https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request" target="_blank" rel="noopener"><code>process_request()</code></a> should either: return <code>None</code>, return a <a href="https://docs.scrapy.org/en/latest/topics/request-response.html#scrapy.http.Response" target="_blank" rel="noopener"><code>Response</code></a> object, return a <a href="https://docs.scrapy.org/en/latest/topics/request-response.html#scrapy.http.Request" target="_blank" rel="noopener"><code>Request</code></a>object, or raise <a href="https://docs.scrapy.org/en/latest/topics/exceptions.html#scrapy.exceptions.IgnoreRequest" target="_blank" rel="noopener"><code>IgnoreRequest</code></a>.</p><p>If it returns a <a href="https://docs.scrapy.org/en/latest/topics/request-response.html#scrapy.http.Response" target="_blank" rel="noopener"><code>Response</code></a> object, Scrapy won’t bother calling <em>any</em> other <a href="https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request" target="_blank" rel="noopener"><code>process_request()</code></a> or <a href="https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception" target="_blank" rel="noopener"><code>process_exception()</code></a> methods, or the appropriate download function; it’ll return that response. </p></li></ul></blockquote><p>当我们重写了这个process_request()方法, 如果返回值是一个Response对象,Scrapy就不会再调用其他的方法, 而是直接返回这个Response对象. 所以这里思路就很清晰了, 大致代码如下:</p><pre class=" language-python"><code class="language-python"><span class="token keyword">from</span> selenium <span class="token keyword">import</span> webdriver<span class="token keyword">from</span> scrapy<span class="token punctuation">.</span>http<span class="token punctuation">.</span>response<span class="token punctuation">.</span>html <span class="token keyword">import</span> HtmlResponse<span class="token keyword">class</span> <span class="token class-name">SeleniumDownloadMiddleware</span><span class="token punctuation">(</span>object<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>driver <span class="token operator">=</span> webdriver<span class="token punctuation">.</span>Chrome<span class="token punctuation">(</span>executable_path<span class="token operator">=</span><span class="token string">"chrome_driver_path"</span><span class="token punctuation">)</span>          <span class="token comment" spellcheck="true">#初始化driver</span>    <span class="token keyword">def</span> <span class="token function">process_request</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> request<span class="token punctuation">,</span> spider<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> request<span class="token punctuation">.</span>meta<span class="token punctuation">[</span><span class="token string">'Function'</span><span class="token punctuation">]</span> <span class="token operator">==</span> <span class="token string">"Ajax"</span><span class="token punctuation">:</span>            <span class="token keyword">return</span> None        self<span class="token punctuation">.</span>driver<span class="token punctuation">.</span>get<span class="token punctuation">(</span>request<span class="token punctuation">.</span>url<span class="token punctuation">)</span>   <span class="token comment" spellcheck="true">#打开网页</span>        source <span class="token operator">=</span> self<span class="token punctuation">.</span>driver<span class="token punctuation">.</span>page_source  <span class="token comment" spellcheck="true">#获取网页源码</span>        response <span class="token operator">=</span> HtmlResponse<span class="token punctuation">(</span>url<span class="token operator">=</span>self<span class="token punctuation">.</span>driver<span class="token punctuation">.</span>current_url<span class="token punctuation">,</span> body<span class="token operator">=</span>source<span class="token punctuation">,</span> request<span class="token operator">=</span>request<span class="token punctuation">,</span> encoding<span class="token operator">=</span><span class="token string">'utf-8'</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> response  <span class="token comment" spellcheck="true">#返回response,这样就会跳过DOWNLOAD模块</span></code></pre><p>这里要注意的是,由于之前在Spider中我们有先向网页发送请求获取文章id组成Ajax请求的url, 这个Request是不能截断的, 所以之前在那个请求中添加的meta就在这里起作用. 当这里返回None时,Scrapy就会跳过这个process_request()方法继续向DOWMLOAD模块请求数据.</p><h4 id="数据存储到MongoDB"><a href="#数据存储到MongoDB" class="headerlink" title="数据存储到MongoDB"></a>数据存储到MongoDB</h4><p>使用Scrapy时, 需要在<code>pipeline.py</code>中处理数据的存储, 这里选取<code>MongoDB</code>作为存储的数据库.推荐使用 <code>from_crawler()</code>类方法完成MongoDB配置参数的传递.</p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> pymongo<span class="token keyword">class</span> <span class="token class-name">JianshuWebMongodbPipeline</span><span class="token punctuation">(</span>object<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>mongo_host<span class="token punctuation">,</span>mongo_port<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>mongo_host <span class="token operator">=</span> mongo_host        self<span class="token punctuation">.</span>mongo_port <span class="token operator">=</span> mongo_port    @classmethod    <span class="token keyword">def</span> <span class="token function">from_crawler</span><span class="token punctuation">(</span>cls<span class="token punctuation">,</span> crawler<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> cls<span class="token punctuation">(</span>            mongo_host<span class="token operator">=</span>crawler<span class="token punctuation">.</span>settings<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">'MMongodb_Host'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            mongo_port<span class="token operator">=</span>crawler<span class="token punctuation">.</span>settings<span class="token punctuation">.</span>get<span class="token punctuation">(</span><span class="token string">'Mongodb_Port'</span><span class="token punctuation">,</span> <span class="token string">'27017'</span><span class="token punctuation">)</span>        <span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">#...</span></code></pre><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>这次实战主要是对简书网的文章异步加载请求进行分析, 从而不断地获取隐藏的文章,真正意义上完成整站爬虫. 诚然用这样的方法会很麻烦, 其实使用Scrapy的CrawlSpider方法会很轻松地爬取整站文章, 只需要通过正则表达式制定url的规则即可爬取整站的文章,并且可以很轻松地避免Ajax的请求分析. </p><p> 当然,还有一些非常实用的Scrapy内置功能没有在这里体现,后续的实战项目会一一地将其部署.</p>]]></content>
      
      
      <categories>
          
          <category> 实战 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Scrapy框架 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Scrapy的另一种启动方式&amp;启动过程探究</title>
      <link href="/2019/02/13/Scrapy%E7%9A%84%E5%8F%A6%E4%B8%80%E7%A7%8D%E5%90%AF%E5%8A%A8%E6%96%B9%E5%BC%8F-%E5%90%AF%E5%8A%A8%E8%BF%87%E7%A8%8B%E6%8E%A2%E7%A9%B6/"/>
      <url>/2019/02/13/Scrapy%E7%9A%84%E5%8F%A6%E4%B8%80%E7%A7%8D%E5%90%AF%E5%8A%A8%E6%96%B9%E5%BC%8F-%E5%90%AF%E5%8A%A8%E8%BF%87%E7%A8%8B%E6%8E%A2%E7%A9%B6/</url>
      
        <content type="html"><![CDATA[<p>Twisted异步</p><blockquote><p>由Scrapy的启动方式到Twisted的reactor对象的深入了解</p></blockquote><h4 id="Crawler-API"><a href="#Crawler-API" class="headerlink" title="Crawler API"></a>Crawler API</h4><p>在我之前的所有使用scrapy框架构建的爬虫中, 都是使用如下方式启动的:</p><blockquote><pre class=" language-python"><code class="language-python"><span class="token keyword">from</span> scrapy <span class="token keyword">import</span> cmdlinecmdline<span class="token punctuation">.</span>execute<span class="token punctuation">(</span><span class="token string">"scrapy crawl spider_name"</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre></blockquote><p>即在.py文件中执行命令行指令, 不过在参考了一些开源的爬虫项目的时候我发现了另外一种方式:</p><blockquote><pre class=" language-python"><code class="language-python"><span class="token keyword">from</span> scrapy<span class="token punctuation">.</span>crawler <span class="token keyword">import</span> CrawlerProcess<span class="token keyword">from</span> scrapy<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>project <span class="token keyword">import</span> get_project_settings<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">"__main__"</span><span class="token punctuation">:</span>    process <span class="token operator">=</span> CrawlerProcess<span class="token punctuation">(</span>get_project_settings<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    process<span class="token punctuation">.</span>crawl<span class="token punctuation">(</span><span class="token string">'spider_name'</span><span class="token punctuation">)</span>    process<span class="token punctuation">.</span>start<span class="token punctuation">(</span><span class="token punctuation">)</span></code></pre></blockquote><p>我们都知道Scrapy内部有部署5大核心模块, 每个模块分工合作来完成整体数据爬取+分析+存储的工作, 但是并不能看出Scrapy是怎么启动的。上段代码在Pycharm中可以直接运行来开启爬虫, 那么这段代码的逻辑又是什么呢? </p><h4 id="CrawlerProcess"><a href="#CrawlerProcess" class="headerlink" title="CrawlerProcess()"></a>CrawlerProcess()</h4><p>对该类的描述如下:<code>A class to run multiple scrapy crawlers in a process simultaneously.</code>即在单进程中运行多个爬虫. 可以看到该类下还有实现 <code>_signal_shutdown()</code>方法, 用于接收<code>Ctrl+c</code> 信号关闭当前爬虫程序. 同时该类继承自<code>CrawlerRunner</code>.</p><h4 id="CrawlerRunner"><a href="#CrawlerRunner" class="headerlink" title="CrawlerRunner()"></a>CrawlerRunner()</h4><p>描述是: <code>This is a convenient helper class that keeps track of, manages and runs</code><br><code>crawlers inside an already setup Twisted reactor_.</code> 用于跟踪、管理、运行爬虫在reactor中.</p><h4 id="get-project-settings"><a href="#get-project-settings" class="headerlink" title="get_project_settings()"></a>get_project_settings()</h4><p>返回值为Settings(), 描述如下: <code>This object stores Scrapy settings for the configuration of internalcomponents, and can be used for any further customization.</code>就是加载spider的配置</p><h4 id="process-crawl-‘spider-name’"><a href="#process-crawl-‘spider-name’" class="headerlink" title="process.crawl(‘spider_name’)"></a>process.crawl(‘spider_name’)</h4><p>该方法位于<code>CrawlerRunner</code>下, 其描述为:<code>Run a crawler with the provided arguments</code> 即在给定参数条件下执行一个爬虫事件.</p><h4 id="process-start"><a href="#process-start" class="headerlink" title="process.start()"></a>process.start()</h4><p>最后一行的start()方法是这么描述的:<code>This method starts a Twisted reactor_, adjusts its pool size to:setting:REACTOR_THREADPOOL_MAXSIZE, and installs a DNS cache based</code><br><code>on :setting:DNSCACHE_ENABLED and :setting:DNSCACHE_SIZE</code>  对其做的工作总结如下:</p><ul><li>创建了一个Twisted reactor对象</li><li>调整pool即线程池的大小</li><li>安装DNS缓存</li><li>判断是否还有爬虫未结束, 并等待其结束再开启当前爬虫</li></ul><p>准备工作完成后执行<code>reactor.run(installSignalHandlers=False)</code>. 看到这里就没有了, 还是没有弄清楚是如何启动程序的,那么这个reactor是怎么生成的?</p><p>追溯到代码的描述, 看来需要了解这个Twisted框架是如何生成reactor对象了.</p><h4 id="Twisted-reactor对象"><a href="#Twisted-reactor对象" class="headerlink" title="Twisted reactor对象"></a>Twisted reactor对象</h4><p>根据文档描述, 这个reactor 是Twisted框架中的事件循环. 这个循环使用Twisted驱动应用程序.reactor本身是提供用于网络、线程、分派事件等的API. </p><p>参考: <a href="https://likebeta.gitbooks.io/twisted-intro-cn/content/zh/p02.html" target="_blank" rel="noopener">https://likebeta.gitbooks.io/twisted-intro-cn/content/zh/p02.html</a></p><p>reactor本身是一个模式, 这个模式本身的存在意义就是不断地等待事件的分配然后去处理该事件, 这样看来, 在Scrapy中reactor仅仅是执行即将分配的爬虫任务而已, 准备工作在之前就已经完成.</p><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>在上述示例中启动一个Scrapy程序的过程中分为以下步骤:</p><ul><li>get_project_settings()读取配置Settings</li><li>CrawlerProcess()类初始化, 并传递Settings信息. 同时初始化shutdown信号处理机制(Ctrl + c)</li><li>生成执行爬虫事件(crawl方法)</li><li>运行CrawlerProcess.start(),分别完成以下任务:<ul><li>创建一个Twisted reactor对象</li><li>调整该对象线程池大小</li><li>创建DNS缓存</li><li>判断是否还有爬虫未结束, 并等待其结束</li><li>运行reactor.run(), 将之前加载完成的spider放在reactor模式运行</li></ul></li></ul><p>上面步骤完成后, Scrapy就会为每个URL创建<code>scrapy.Request</code>对象，并将默认的<code>parse()</code>方法作为<code>scrapy.Request</code>对象的<code>callback</code>回调函数。</p>]]></content>
      
      
      <categories>
          
          <category> Twisted </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Scrapy框架 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>破解有道翻译js反爬虫机制</title>
      <link href="/2019/02/09/%E7%A0%B4%E8%A7%A3%E6%9C%89%E9%81%93%E7%BF%BB%E8%AF%91js%E5%8F%8D%E7%88%AC%E8%99%AB%E6%9C%BA%E5%88%B6/"/>
      <url>/2019/02/09/%E7%A0%B4%E8%A7%A3%E6%9C%89%E9%81%93%E7%BF%BB%E8%AF%91js%E5%8F%8D%E7%88%AC%E8%99%AB%E6%9C%BA%E5%88%B6/</url>
      
        <content type="html"><![CDATA[<blockquote><p>解读有道翻译网页内部js代码, 构建反爬虫应对措施</p></blockquote><h2 id="常规爬虫"><a href="#常规爬虫" class="headerlink" title="常规爬虫"></a>常规爬虫</h2><p>点开有道翻译的链接: <a href="http://fanyi.youdao.com/" target="_blank" rel="noopener">http://fanyi.youdao.com/</a>, 然后任意输入需要翻译的内容, 这里以输入:<code>life is short, I choose python</code> 为例. 在界面中 <code>右键-&gt;检查-&gt;打开Network</code> 查看所有的网络请求:</p><p><img src="/2019/02/09/破解有道翻译js反爬虫机制/1.png" alt=""></p><p><img src="/2019/02/09/破解有道翻译js反爬虫机制/2.png" alt="2"></p><p>可以看到这个请求是获取翻译的response结果的. 需要一个POST请求. 然后查看其需要传递的Form data:</p><p><img src="/2019/02/09/破解有道翻译js反爬虫机制/3.png" alt=""></p><p>对比多次请求的Form Data, 发现有几个需要关注的数据:</p><ul><li>i: 需要翻译的字符串</li><li>salt: 值是变化的, 应该是加密用到的”盐”</li><li>sign: 签名字符串, 也是变化的</li><li>ts: 与timestamp时间戳很像,也是不断变化</li></ul><p>其他的数据则保持不变, 那么这里先进行一个初步的尝试:</p><blockquote><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> requeststrans_word <span class="token operator">=</span> input<span class="token punctuation">(</span><span class="token string">"请输入需要翻译的单词:"</span><span class="token punctuation">)</span>url <span class="token operator">=</span> <span class="token string">"http://fanyi.youdao.com/translate_o?smartresult=dict&amp;smartresult=rule"</span>headers <span class="token operator">=</span> <span class="token punctuation">{</span> <span class="token string">"Cookie"</span><span class="token punctuation">:</span> <span class="token string">"OUTFOX_SEARCH_USER_ID=-1548144101@10.168.8.76;JSESSIONID=aaaTLWzfvp5Hfg9mAhFkw;OUTFOX_SEARCH_USER_ID_NCOO=1999296830.4784973;___rl__test__cookies=1523100789517"</span><span class="token punctuation">,</span> <span class="token string">"Origin"</span><span class="token punctuation">:</span> <span class="token string">"http://fanyi.youdao.com"</span><span class="token punctuation">,</span> <span class="token string">"Referer"</span><span class="token punctuation">:</span> <span class="token string">"http://fanyi.youdao.com/"</span><span class="token punctuation">,</span> <span class="token string">'User-Agent'</span><span class="token punctuation">:</span> <span class="token string">'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36'</span><span class="token punctuation">}</span>data_ <span class="token operator">=</span> <span class="token punctuation">{</span> <span class="token string">"i"</span><span class="token punctuation">:</span> trans_word<span class="token punctuation">,</span> <span class="token string">"from"</span><span class="token punctuation">:</span> <span class="token string">"AUTO"</span><span class="token punctuation">,</span> <span class="token string">"to"</span><span class="token punctuation">:</span> <span class="token string">"AUTO"</span><span class="token punctuation">,</span> <span class="token string">"smartresult"</span><span class="token punctuation">:</span> <span class="token string">"dict"</span><span class="token punctuation">,</span> <span class="token string">"client"</span><span class="token punctuation">:</span> <span class="token string">"fanyideskweb"</span><span class="token punctuation">,</span> <span class="token string">"salt"</span><span class="token punctuation">:</span> <span class="token string">"15496907378368"</span><span class="token punctuation">,</span> <span class="token string">"sign"</span><span class="token punctuation">:</span> <span class="token string">"f6ebbfde3f572df78882abf344940971"</span><span class="token punctuation">,</span> <span class="token string">"ts"</span><span class="token punctuation">:</span> <span class="token string">"1549690737836"</span><span class="token punctuation">,</span> <span class="token string">"bv"</span><span class="token punctuation">:</span> <span class="token string">'6f014bd66917f921835d1d6ae8073eb1'</span><span class="token punctuation">,</span> <span class="token string">"doctype"</span><span class="token punctuation">:</span> <span class="token string">"json"</span><span class="token punctuation">,</span> <span class="token string">"version"</span><span class="token punctuation">:</span> <span class="token string">"2.1"</span><span class="token punctuation">,</span> <span class="token string">"keyfrom"</span><span class="token punctuation">:</span> <span class="token string">"fanyi.web"</span><span class="token punctuation">,</span> <span class="token string">"action"</span><span class="token punctuation">:</span> <span class="token string">"FY_BY_CLICKBUTTION"</span><span class="token punctuation">,</span> <span class="token string">"typoResult"</span><span class="token punctuation">:</span> <span class="token string">"false"</span><span class="token punctuation">}</span>translate_res <span class="token operator">=</span> requests<span class="token punctuation">.</span>post<span class="token punctuation">(</span>url<span class="token punctuation">,</span> headers<span class="token operator">=</span>headers<span class="token punctuation">,</span> data<span class="token operator">=</span>data_<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>translate_res<span class="token punctuation">.</span>json<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre></blockquote><p>运行后的结果如下:</p><pre><code>{&#39;errorCode&#39;: 50}</code></pre><p>恩? 得到一个错误代码, 于是我不断尝试发现, 当输入一个单词及其对应的请求数据是可以成功的, 不可能说有道翻译只能翻译一个单词吧. 看来应该是有道的反爬虫机制了.</p><h2 id="破解反爬虫机制"><a href="#破解反爬虫机制" class="headerlink" title="破解反爬虫机制"></a>破解反爬虫机制</h2><p>这里我进行了多次的翻译, 这里拿hi 与 hello 的翻译做比较:</p><p><img src="/2019/02/09/破解有道翻译js反爬虫机制/4.png" alt=""></p><p><img src="/2019/02/09/破解有道翻译js反爬虫机制/5.png" alt="5"></p><p>发现salt、sign、ts这三个数据会随着i的变化而改变, 那么这里分析这些数据的生成方式可能会有两种:</p><ul><li>每次翻译的时候, 有道翻译服务器返回的值</li><li>网页中的JS代码按照一定的规则生成</li></ul><p>如果是第一种情况, 这里我们查看一下当前的其他请求:</p><p><img src="/2019/02/09/破解有道翻译js反爬虫机制/6.png" alt=""></p><p>那么第一种情况就可以排除. 若这串字符是在本地生成的, 那么查看网页的源代码, 查找所有JS文件, 找到了这个fanyi.min.js文件:</p><p><img src="/2019/02/09/破解有道翻译js反爬虫机制/7.png" alt=""></p><p>然后点击Source, 在其中找到这个js文件, 搜索salt:</p><p><img src="/2019/02/09/破解有道翻译js反爬虫机制/8.png" alt=""></p><p>这里简单地梳理一下:</p><ul><li>ts: 当前的时间, 以<code>timestamp</code>时间戳的形式</li><li>bv: 值为<code>navigator.appVersion</code>的<code>md5</code>加密后的值, 这里表示的是使用的浏览器的版本:</li></ul><pre class=" language-javascript"><code class="language-javascript">console<span class="token punctuation">.</span><span class="token function">log</span><span class="token punctuation">(</span><span class="token string">'appVersion= '</span> <span class="token operator">+</span> navigator<span class="token punctuation">.</span>appVersion<span class="token punctuation">)</span></code></pre><pre><code>appVersion = 5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36</code></pre><ul><li>salt: 值为当前时间戳 + 范围为0~9的整数随机数</li><li>sign: 值为”fanyideskweb” + e + i + “p09@Bn{h02_BIEe]$P^nG”. 其中i为salt的值</li></ul><p>那么这个e又是啥呀? 这里我们就添加断点调试这个JS代码一探究竟:</p><p><img src="/2019/02/09/破解有道翻译js反爬虫机制/9.png" alt=""></p><p>发现原来这个 e 就是需要翻译的内容, 那么<code>sign</code>的规律也清楚了, 接下来就是用python来对应其接口. 对于md5算法, python中有专门的库 hashlib, 直接调用即可.</p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> requests<span class="token keyword">import</span> time<span class="token keyword">import</span> random<span class="token keyword">import</span> hashlibtrans_word <span class="token operator">=</span> input<span class="token punctuation">(</span><span class="token string">"请输入需要翻译的单词:"</span><span class="token punctuation">)</span>url <span class="token operator">=</span> <span class="token string">"http://fanyi.youdao.com/translate_o?smartresult=dict&amp;smartresult=rule"</span>headers <span class="token operator">=</span> <span class="token punctuation">{</span>    <span class="token string">"Cookie"</span><span class="token punctuation">:</span>    <span class="token string">"OUTFOX_SEARCH_USER_ID=-1548144101@10.168.8.76;JSESSIONID=aaaTLWzfvp5Hfg9mAhFkw;OUTFOX_SEARCH_USER_ID_NCOO=1999296830.4784973;___rl__test__cookies=1523100789517"</span><span class="token punctuation">,</span>    <span class="token string">"Origin"</span><span class="token punctuation">:</span>    <span class="token string">"http://fanyi.youdao.com"</span><span class="token punctuation">,</span>    <span class="token string">"Referer"</span><span class="token punctuation">:</span>    <span class="token string">"http://fanyi.youdao.com/"</span><span class="token punctuation">,</span>    <span class="token string">'User-Agent'</span><span class="token punctuation">:</span>    <span class="token string">'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36'</span><span class="token punctuation">}</span>ts <span class="token operator">=</span> str<span class="token punctuation">(</span>int<span class="token punctuation">(</span>time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">1000</span><span class="token punctuation">)</span><span class="token punctuation">)</span>salt <span class="token operator">=</span> int<span class="token punctuation">(</span>time<span class="token punctuation">.</span>time<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token number">10000</span><span class="token punctuation">)</span> <span class="token operator">+</span> random<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>md5 <span class="token operator">=</span> hashlib<span class="token punctuation">.</span>md5<span class="token punctuation">(</span><span class="token punctuation">)</span>md5<span class="token punctuation">.</span>update<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token string">'fanyideskweb'</span> <span class="token operator">+</span> trans_word <span class="token operator">+</span> str<span class="token punctuation">(</span>salt<span class="token punctuation">)</span> <span class="token operator">+</span>            <span class="token string">"ebSeFb%=XZ%T[KZ)c(sy!"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>encode<span class="token punctuation">(</span><span class="token string">'utf-8'</span><span class="token punctuation">)</span><span class="token punctuation">)</span>sign <span class="token operator">=</span> md5<span class="token punctuation">.</span>hexdigest<span class="token punctuation">(</span><span class="token punctuation">)</span>data_ <span class="token operator">=</span> <span class="token punctuation">{</span>    <span class="token string">"i"</span><span class="token punctuation">:</span> trans_word<span class="token punctuation">,</span>    <span class="token string">"from"</span><span class="token punctuation">:</span> <span class="token string">"AUTO"</span><span class="token punctuation">,</span>    <span class="token string">"to"</span><span class="token punctuation">:</span> <span class="token string">"AUTO"</span><span class="token punctuation">,</span>    <span class="token string">"smartresult"</span><span class="token punctuation">:</span> <span class="token string">"dict"</span><span class="token punctuation">,</span>    <span class="token string">"client"</span><span class="token punctuation">:</span> <span class="token string">"fanyideskweb"</span><span class="token punctuation">,</span>    <span class="token string">"salt"</span><span class="token punctuation">:</span> salt<span class="token punctuation">,</span>    <span class="token string">"sign"</span><span class="token punctuation">:</span> sign<span class="token punctuation">,</span>    <span class="token string">"ts"</span><span class="token punctuation">:</span> ts<span class="token punctuation">,</span>    <span class="token string">"bv"</span><span class="token punctuation">:</span> <span class="token string">'6f014bd66917f921835d1d6ae8073eb1'</span><span class="token punctuation">,</span>    <span class="token string">"doctype"</span><span class="token punctuation">:</span> <span class="token string">"json"</span><span class="token punctuation">,</span>    <span class="token string">"version"</span><span class="token punctuation">:</span> <span class="token string">"2.1"</span><span class="token punctuation">,</span>    <span class="token string">"keyfrom"</span><span class="token punctuation">:</span> <span class="token string">"fanyi.web"</span><span class="token punctuation">,</span>    <span class="token string">"action"</span><span class="token punctuation">:</span> <span class="token string">"FY_BY_CLICKBUTTION"</span><span class="token punctuation">,</span>    <span class="token string">"typoResult"</span><span class="token punctuation">:</span> <span class="token string">"false"</span><span class="token punctuation">}</span>translate_res <span class="token operator">=</span> requests<span class="token punctuation">.</span>post<span class="token punctuation">(</span>url<span class="token punctuation">,</span> headers<span class="token operator">=</span>headers<span class="token punctuation">,</span> data<span class="token operator">=</span>data_<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>translate_res<span class="token punctuation">.</span>json<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><h2 id="一段小结"><a href="#一段小结" class="headerlink" title="一段小结"></a>一段小结</h2><p>像如上的示例, 通过JS代码在本地生成一定规律的字符串, 也是启发了自己一个新的应对反爬虫机制的思路. 相比较于一些基本的反爬概念(随机请求头、代理IP池、访问频率限制), 这样的方式更像是通过网页本身去挖掘信息并解析. 后续也会去找一些更精彩的反爬虫实例, 在这里记录我的每一步. </p>]]></content>
      
      
      <categories>
          
          <category> 实战 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python网络爬虫 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2019/02/05/hello-world/"/>
      <url>/2019/02/05/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre class=" language-bash"><code class="language-bash">$ hexo new <span class="token string">"My New Post"</span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre class=" language-bash"><code class="language-bash">$ hexo server</code></pre><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre class=" language-bash"><code class="language-bash">$ hexo generate</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre class=" language-bash"><code class="language-bash">$ hexo deploy</code></pre><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
